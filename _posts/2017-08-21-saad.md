---
layout: post
title: "Implementing Apache Flume Sink"
date: 2017-08-21
---

With rapid developments in the computing universe, the software environment is becoming more and
more distributed and heterogeneous. To address the compatibility issue, software applications called
middleware are used to connect two disparate applications that have no inherent methods of their own
to communicate between themselves.
With the advent and immense popularity of social media systems, much research is being conducted to
develop efficient and fast middleware programs to translate and transfer huge amounts of multimedia
data. Apache Flume is one such way to bring social media data into HDFS. Flume is described as “a
distributed, reliable, and available service for efficiently collecting, aggregating, and moving large
amounts of log data.” It is a robust and fault tolerant middleware that uses a simple extensible data
model that allows for online applications. At the most basic level, Flume enables applications to collect
data from its origin and send it to a resting location, such as HDFS. At a slightly more detailed level,
Flume achieves this goal by defining data flows consisting of three primary structures: sources,
channels and sinks. The pieces of data that flow through Flume are called events, and the processes that
run the dataflow are called agents. Our selection of flume is due to its ability to cater for streaming data
flows that includes, but not restricted to, social media data, multimedia data, transactional data and
financial data.
The report is organized as follows, section II contains steps and descriptions for setting up the Hadoop
and the cluster. After the environment is setup, we are ready to configure our Twitter API that will act
as our source of data in section III. After all the foundation is laid, we setup flume in Section IV and
connect it with the live streaming data source of Twitter API. Section V discusses running the
streaming data service and loading social media data into HDFS. Section VI concludes this project
report and reviews the findings and future avenues of research.

II. Setting up Hadoop & the Cluster

The Apache Hadoop software library is a framework that allows for the distributed processing of large
data sets across clusters of computers using simple programming models. It is designed to scale up
from single servers to thousands of machines, each offering local computation and storage. The core of
Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a
processing part called MapReduce. The Hadoop Distributed File System (HDFS) is a distributed file
system that is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS splits
files into large blocks and distributes them across nodes in a cluster.

Our Hadoop setup is deployed in the form of a virtual cluster. This deployment setup is the norm in

development environments where programmers run their test scripts and code on their virtually-
distributed environment. This mode of installation uses multiple virtual Hadoop instances

simultaneously as containers. This allows us to quickly write scripts and run them on our test data
without having to worry about system crashes or configuration failures. Following are the steps we
performed to setup the cluster on virtually-distributed settings.

(a) Setting up Linux
Prior to the installation of hadoop on our cluster we need to have a Linux operating system running,
configured and ready for HDFS. We will be using Ubuntu x64 14.04 LTS as our base operating system.
This is a native installation and we have not used any dual boot or VMWare based OS. Native
installations allow maximal control over the environment and this is necessary while running
experimental development software like Apache Flume.
After the initial installation, first we make sure that the system is fully up-to-date with the required by
running the following commands:

~$ sudo apt-get update && sudo apt-get upgrade
~$ sudo apt-get install build-essential ssh lzop git rsync curl
~$ sudo apt-get install python-dev python-setuptools
~$ sudo apt-get install libcurl4-openssl-dev
~$ sudo easy_install pip
~$ sudo pip install virtualenv virtualenvwrapper python-dateutil

Installing Java
Hadoop and most of the Hadoop ecosystem require Java to run. Hadoop requires a minimum of Oracle
Java 1.6.x or greater and used to recommend particular versions of Java to use with Hadoop. Now,
Hadoop maintains a reporting of the various JDKs that work well with Hadoop. Ubuntu does not
maintain an Oracle JDK in Ubuntu repositories because it is proprietary code, so instead we will install
OpenJDK by running the following command in the Ubuntu command line.
~$ sudo apt-get install openjdk-7-*

Perform a quick check to ensure the right version of Java is installed by running the following
command. Hadoop is currently built and tested on both OpenJDK and Oracle's JDK/JRE.
~$ java -version
java version "1.7.0_65"
OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)
Disabling IPv6
It has been reported for a while now that Hadoop running on Ubuntu has a conflict with IPv6, and ever
since Hadoop 0.20, Ubuntu users have been disabling IPv6 on their clustered boxes. It is unclear
whether or not this is still a bug in the latest versions of Hadoop, however in a virtually-distributed
environment we will have no need for IPv6, so it is best to simply disable it and not worry about any
potential problems. We will edit the sysctl.conf file which is location in the /etc/sysctl folder by
running the following lines of code
~$ gksu gedit /etc/sysctl.conf

We will append the following lines to the end of the sysctl file

# disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

For this change to take effect we need to perform a reboot. Once the computer has rebooted we need to
check the status with the following command:
~$ cat /proc/sys/net/ipv6/conf/all/disable_ipv6

If the output is 0, then IPv6 is enabled. If it is 1, then we have successfully disabled IPv6.

Creating a Hadoop User
In order to secure our Hadoop services, we will make sure that Hadoop is run as a Hadoop-specific user
and group. This user would be able to initiate SSH connections to other nodes in a cluster, but not have
administrative access to do damage to the operating system upon which the service was running.
Implementing Linux permissions also helps secure HDFS and is the start of preparing a secure
computing cluster. This will also ensure that the Hadoop installation is separate from other software
applications and will help organize the maintenance of the machine.
To create a separate hadoop user and group, we run the following commands in the terminal, then add
the student user to the Hadoop group:
~$ sudo addgroup hadoop
~$ sudo useradd -m -g hadoop hadoop
~$ sudo usermod -a -G hadoop student

Note that the -r flag creates a system user without a home directory. After these steps, we need to
perform a restart to see that the student user has been added to the Hadoop group by issuing the groups
command.

Configuring SSH
SSH is required and must be installed on the system to use Hadoop (and to better manage the virtual
environment, especially if you're using a headless Ubuntu). We start by generating ssh keys for the
Hadoop user by issuing the following commands:
~$ sudo su hadoop
~$ ssh-keygen

This will generate a unique public/private rsa key pair and require to enter file in which to save the key.
The default is selected to be (/home/hadoop/.ssh/id_rsa). We will leave this value blank and hit the
enter key to leave it unchanged. A directory will be created by the address '/home/hadoop/.ssh' and the
keygen will ask to enter a passphrase to secure the access and usage of this rsa key pair. We do not
want to assign any passphrase since this is a local virtually-distributed installation so we are not at the
danger of being exploited by network intruders. The keygen will then again ask to repeat the
passphrase. Hit enter again to leave it unchanged and unassigned. We see the following message as a
confirmation that the rsa id has been created and saved.
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.

We have just created a rsa key that does not require a password to access hadoop . In order to allow the
key to be used to SSH into the box, we copy the public key to the authorized_keys file with the
following command:
~$ cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys
~$ chmod 600 /home/hadoop/.ssh/authorized_keys

We will now download this key and use it to SSH into the Ubuntu environment. To test the SSH key
we issue the following command:
~$ ssh -l hadoop localhost

This test completes successfully without asking for a password, thus confirming that we have
successfully configured SSH for Hadoop. Exiting the SSH window by typing exit and returning back to 
the hadoop user. Exit the Hadoop user by typing exit again, we will reach a terminal window that says
student@ubuntu where student is the username and @ubuntu is the name of our computer.

(b) Installing Hadoop
To get Hadoop, we will need to download the latest stable release of from one of the Apache Download
Mirrors. The following steps will download the current stable version of Hadoop with YARN.
Although Hadoop 2.7.2 has been released but we encountered several issues when using Flume with
Hadoop 2.7.2, thus we have opted for the reportedly stable version of Hadoop 2.5.2 that also works
perfect with Flume.
After we decide on the mirror closest to our location, we run the following command in the terminal
window to download the Hadoop binary.
~/Downloads$ curl -O http://apache.claz.org/hadoop/common/hadoop-2.5.2/hadoop-2.5.2.tar.gz

Ater the download, we verify the download by ensuring that the md5sum matches the md5sum which
should also be available at the mirror:
~/Downloads$ md5sum hadoop-2.5.2.tar.gz
74a7581893a8224540a9417a4c2630da hadoop-2.5.2.tar.gz

Unpacking
After obtaining the compressed tarball, the next step is to unpack it. One of the most important
decisions is to decide where to unpack Hadoop. The Linux operating system depends upon a
hierarchical directory structure to function. At the root, many directories have specific purposes as
described below
/etc is used to store configuration files
/home is used to store user specific files
/bin and /sbin include programs that are vital for the OS
/usr/sbin are for programs that are not vital but are system wide
/usr/local is for locally installed programs
/var is used for program data including caches and logs

From observing different implementations from other users, it was deduced that there are two good
choices to move Hadoop, that are /opt and /srv directories. /opt contains non-packaged programs,
usually source. A lot of developers stick their code there for deployments.
The /srv directory stands for services. Hadoop, HBase, Hive and others run as services on your
machine. We will Hdoop here and it's a standard location that's easy to get to. We use the following
commands to unpack Hadoop to /srv:
~/Downloads$ tar -xzf hadoop-2.5.2.tar.gz
~/Downloads$ sudo mv hadoop-2.5.2 /srv/
~/Downloads$ sudo chown -R hadoop:hadoop /srv/hadoop-2.5.2
~/Downloads$ sudo chmod g+w -R /srv/hadoop-2.5.2
~/Downloads$ sudo ln -s /srv/hadoop-2.5.2 /srv/hadoop

These commands unpack Hadoop, move it to the service directory where we will keep all of our
Hadoop and cluster services, and then set permissions. Finally, we create a symlink to the version of
Hadoop that we would like to use, this will make it easy to upgrade our Hadoop distribution in the
future.

Setting Environment Variables
In order to ensure everything executes correctly, we are going to set some environment variables so
that Hadoop executes in its correct context. By using the following commands we open up a text editor
with the profile of the hadoop user to change the environment variables and add the following lines to
this file:
/srv$ gksu gedit /home/hadoop/.bashrc

# Set the Hadoop Related Environment variables
export HADOOP_HOME=/srv/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
# Set the JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64

We'll also add some convenience functionality to the student user environment by opening the student
user bash profile file with the following command and adding the following contents to that file:

~$ gedit ~/.profile
# Set the Hadoop Related Environment variables
export HADOOP_HOME=/srv/hadoop

export HADOOP_STREAMING=$HADOOP_HOME/share/hadoop/tools/lib/hadoop streaming-
2.5.2.jar

export PATH=$PATH:$HADOOP_HOME/bin
# Set the JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
# Helpful Aliases
alias ..="cd .."
alias ...="cd ../.."
alias hfs="hadoop fs"
alias hls="hfs -ls"

These simple aliases will save us a lot of typing in the long run. To review the changes we check that
the environment configuration has worked by running a Hadoop command:
~$ hadoop version
Result >
Hadoop 2.5.2
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r
cc72e9b000545b86b75a61f4835eb86d57bfafc0
Compiled by jenkins on 2014-11-14T23:45Z
Compiled with protoc 2.5.0
From source with checksum df7537a4faa4658983d397abf4514320
This command was run using /srv/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar

If that ran with no errors and displayed an output similar to the one above, then everything has been
configured correctly up to this point.

Hadoop Configuration
The next step is to setup Hadoop as a virtually-distributed node. This is achieved by editing the
configuration files of
1. The Hadoop environment
2. The Mapreduce site
3. The HDFS site
4. The Yarn site.

We start by editing the hadoop-env.sh file by entering the following on the command line.
~$ gedit $HADOOP_HOME/etc/hadoop/hadoop-env.sh

The most important part of this configuration is to change the following line:
# The java implementation to use.
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64

Next, we edit the core site configuration file:
~$ gedit $HADOOP_HOME/etc/hadoop/core-site.xml

In this file we replace the <configuration></configuration> tags with the following code:
<configuration>
<property>
<name>fs.default.name</name>

<value>hdfs://localhost:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/var/app/hadoop/data</value>
</property>
</configuration>

Following the core-site.xml, we edit the MapReduce site configuration by making a copy of the
original and then opening the file for editing:
~$ cp $HADOOP_HOME/etc/hadoop/mapred-site.xml.template \
$HADOOP_HOME/etc/hadoop/mapred-site.xml

~$ gedit $HADOOP_HOME/etc/hadoop/mapred-site.xml

We will replace the <configuration></configuration> tags with the following changes:
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>
Similarly, we edit the HDFS site configuration by editing the following file:
~$ gedit $HADOOP_HOME/etc/hadoop/hdfs-site.xml

We replace the <configuration></configuration> with the following code snippet:
<configuration>
<property>
<name>dfs.replication</name>

<value>1</value>
</property>
</configuration>
Finally, we edit the YARN site configuration file:
~$ gedit $HADOOP_HOME/etc/hadoop/yarn-site.xml

And update the configuration as follows:
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>localhost:8025</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>localhost:8030</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>localhost:8050</value>
</property>
</configuration>
With these files edited, Hadoop should be fully configured as a virtually-distributed environment.

Formatting the Namenode
The final step before we can turn Hadoop on is to format the 'namenode'. The namenode is in charge of
HDFS, the distributed file system. The namenode on this machine is going to keep its files in the
/var/app/hadoop/data directory. We need to initialize this directory and then format the namenode to
properly use it. We open a new terminal and run the following commands
~$ sudo mkdir -p /var/app/hadoop/data
~$ sudo chown hadoop:hadoop -R /var/app/hadoop
~$ sudo su hadoop
~$ hadoop namenode -format
The system will generate several Java messages if the namenode has executed successfully. There
should be directories inside of the /var/app/hadoop/data directory, including a dfs directory. If that is
observed in the terminal, then our Hadoop setup is all set up and ready to use.
Starting Hadoop
At this point we can start and run our Hadoop daemons. When we formatted the namenode, we
switched to being the hadoop user with the sudo su hadoop command. If the same user is still under the
shell, then we go ahead and execute the following commands:
~$ $HADOOP_HOME/sbin/start-dfs.sh
~$ $HADOOP_HOME/sbin/start-yarn.sh

The daemons should start up and issue messages about where they are logging to and other important
information. If the system asks about the SSH key, we will type y at the prompt. To view the running
processes, we run the jps command:
~$ jps
4801 Jps
4468 ResourceManager
4583 NodeManager
4012 NameNode
4318 SecondaryNameNode
4150 DataNode

If the processes are not running, then something has gone wrong. To view the Hadoop cluster
administration site, we open the http://localhost:8088 in any compatible browser. This should bring up
a page similar to the following, with the Hadoop logo and a table of applications.

 <img src="https://flume.apache.org/_images/DevGuide_image00.png" class="img-responsive" alt=""> 


To wrap up the configuration, we prepare a space on HDFS for our student account to store data and to
run analytical jobs on:
~$ hadoop fs -mkdir -p /user/student
~$ hadoop fs -chown student:student /user/student

Restarting Hadoop
If in case of a machine reboot, the Hadoop daemons will stop running and will not automatically be
restarted. If we attempt to run a Hadoop command, we will get an error message "connection refused",
it is likely because the daemons are not running. This can be checked by issuing the jpscommand as
sudo:
~$ sudo jps

To restart Hadoop in the case that it shuts down, issue the following commands:
~$ sudo -H -u hadoop $HADOOP_HOME/sbin/start-dfs.sh

~$ sudo -H -u hadoop $HADOOP_HOME/sbin/start-yarn.sh
The processes should start up again as the dedicated hadoop user and the system be back online and can
be verified by opening the hadoop cluster administration from http://localhost:8088.

III. Setting up Twitter
In this project, the objective was to get data from a social media data source and we have selected
Twitter to be the designated data source in our project. Twitter is a critical data producer and has
recently received much attention from big data researchers. We will try to get Tweets using Flume
acting as a middleware and save them into HDFS. Twitter exposes its API to get the Tweets through
various middlewares, the service is free, but requires the user to register for the service and create a
Twitter application API. We present the following steps to register and create a Twitter API and get the
access codes necessary for the connection.
Create Twitter Application
We start by logging on the dedicated twitter application website https://apps.twitter.com and clicking
on the sign in button and signing in to open our management area.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/2.PNG)

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/3.PNG)

After we sign in to our Twitter account we will have a Twitter Application Management window where
we see, manage or delete our previously created applications and also a create new app button that we
can use to create new Twitter Apps.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/4.PNG)

Create New App
By clicking on the Create New App button we will be redirected to a window where we will get an

application form in which Twitter is requesting rationalization details to create the App. While filling
the website address, it is necessary to give the complete URL pattern, for example,http://example.com.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/5.PNG)


Developer Agreement
After we fill in the details, we follow through by accepting the Developer Agreement and click on the
Create your Twitter application button which is at the bottom of the page.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/6.PNG)


Key & Access Tokens
If everything goes fine, an App will be created with the given details as shown below. We will click the
keys and Access Tokens tab at the top of the page to goto the access control keys area where we can
generate a new access key token for our API.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/7.PNG)


Different Tokens
At the bottom of the page there is a button named Create my access token. Click on it to generate the
access token. This will take us to the Access keys page which displays our Consumer key, Consumer
secret, Access token,and Access token secret. Copy these details. These are useful to configure the
agent in Flume.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/8.PNG)
![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/9.PNG)

By following the above mentioned steps, our Twitter API is setup and we need to move on to
configuring our Middleware application.

IV. Setting up Flume
Flume is a middleware product from the Apache opensource family that lets Hadoop users make the
most of valuable log and streaming data. Specifically, Flume allows users to:

• Stream data from multiple sources into Hadoop for analysis
• Collect high-volume Web logs in real time
• Insulate themselves from transient spikes when the rate of incoming data exceeds the rate at
which data can be written to the destination
• Guarantee data delivery
• Scale horizontally to handle additional data volume
Flume is composed of the following components as shown in fig 4.1:
• Event – a singular unit of data that is transported by Flume (typically a single log entry.
• Source – the entity through which data enters into Flume. Sources either actively poll for data
or passively wait for data to be delivered to them. A variety of sources allow data to be
collected, such as log4j logs and syslogs.
• Sink – the entity that delivers the data to the destination. A variety of sinks allow data to be
streamed to a range of destinations. One example is the HDFS sink that writes events to HDFS.
• Channel – the conduit between the Source and the Sink. Sources ingest events into the channel
and the sinks drain the channel.
• Agent – any physical Java virtual machine running Flume. It is a collection of sources, sinks
and channels.
• Client – produces and transmits the Event to the Source operating within the Agent
Flume middleware workflow start from a webserver generateing log data and this data is collected by
an agent in Flume. The webserver transmits an event to a Source operating within the Agent. The
Source receiving this event then delivers it to one or more Channels. These Channels are drained by
one or more Sinks operating within the same Agent. The channel buffers this data to a sink, which
finally pushes it to centralized stores.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/10.PNG)


Although we can fetch data from various services and transport it to HDFS, in the example we will use
flume to bugger tweets into the generated HDFS sink and push the tweets into the HDFS. To achieve
this we have to configure the source, the channel, and the sink using the configuration file in the flume
conf folder. We will use an experimental source provided by Apache Flume named Twitter 1%
Firehose Memory channel and HDFS sink.
Twitter 1% Firehose Source
This source is highly experimental. It connects to the 1% sample Twitter Firehose using streaming API
and continuously downloads tweets, converts them to Avro format, and sends Avro events to a
downstream Flume sink. We will get this source by default along with the installation of Flume. The
jar files corresponding to this source can be located in the lib folder as shown in Figure 4.2.

Setting the classpath
We need to set the classpath variable to the lib folder of Flume in Flume-env.sh file by running the
following command.
export CLASSPATH=$CLASSPATH:/FLUME_HOME/lib/*

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/11.PNG)

This source needs the details such as Consumer key, Consumer secret, Access token,and Access token
secret of a Twitter application. While configuring this source, we have to provide values to the
following properties −
•Channels
•Source type : org.apache.flume.source.twitter.TwitterSource
•consumerKey − The OAuth consumer key
•consumerSecret − OAuth consumer secret
•accessToken − OAuth access token
•accessTokenSecret − OAuth token secret

•maxBatchSize − Maximum number of twitter messages that should be in a twitter batch. The
default value is 1000 (optional).
•maxBatchDurationMillis − Maximum number of milliseconds to wait before closing a batch.
The default value is 1000 (optional).
Channel
We are using the memory channel in this particular project. To configure the memory channel, we will
declare a value to the type of the channel.
•type − It holds the type of the channel. In our example, the type is MemChannel.
•Capacity − It is the maximum number of events stored in the channel. Its default value is 100
(optional).
•TransactionCapacity − It is the maximum number of events the channel accepts or sends. Its
default value is 100 (optional).
HDFS Sink
This sink writes data into the HDFS. To configure this sink, we will provide the following details.
•Channel
•type − hdfs
•hdfs.path − the path of the directory in HDFS where data is to be stored.

And we can provide some optional values based on the scenario. Given below are the optional
properties of the HDFS sink that we are configuring in our application.
•fileType − This is the required file format of our HDFS file. SequenceFile, DataStream and
CompressedStream are the three types available with this stream. In our example, we are using
the DataStream.
•writeFormat − Could be either text or writable.
•batchSize − It is the number of events written to a file before it is flushed into the HDFS. Its
default value is 100.
•rollsize − It is the file size to trigger a roll. It default value is 100.
•rollCount − It is the number of events written into the file before it is rolled. Its default value
is 10.

Configuration File
Given below is an example of the configuration file. Copy this content and save as twitter.conf in the
conf folder of Flume.

# Naming the components on the current agent.
TwitterAgent.sources = Twitter
TwitterAgent.channels = MemChannel
TwitterAgent.sinks = HDFS

# Describing/Configuring the source
TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource
TwitterAgent.sources.Twitter.consumerKey = VZ3tdGkQ8UmBNaGdZlGPbXzDe
TwitterAgent.sources.Twitter.consumerSecret =
P42w3im9slywHCYBn9BrN1G3fV887xL7DpwLW0Y9e5Kz05A3X1
TwitterAgent.sources.Twitter.accessToken = 2481790844-
SLnAUIs9WxRCTv5Eb99xjT8Hq79eyh3cCJfa8xN
TwitterAgent.sources.Twitter.accessTokenSecret =
V5ZLwgvXXsgCTducaesFwOM1N6egD5fRqnHz4omKAyJeS
TwitterAgent.sources.Twitter.keywords = trump, primaries, immigration, jobs, environment, Donald
Trump

# Describing/Configuring the sink
TwitterAgent.sinks.HDFS.type = hdfs
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://localhost:9000/user/Hadoop/twitter_data/
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text
TwitterAgent.sinks.HDFS.hdfs.batchSize = 10
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000
# Describing/Configuring the channel TwitterAgent.channels.MemChannel.type = memory
TwitterAgent.channels.MemChannel.capacity = 10000
TwitterAgent.channels.MemChannel.transactionCapacity = 100

# Binding the source and sink to the channel
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sinks.HDFS.channel = MemChannel

V. Execution, Loading data
Now that we understand the configuration of our source, channel and sink, we need to start up the agent
to get the dataflow running. Before we actually start the agent, it is noteworthy to mention about the
type of tweets we will be receiving.

Keywords Configuration
Since it is the election season and election 2016 tweets are the most high volume messages running on
social media today, we have selected election 2016 related keywords in our configuration file. We have
set the 'TwitterAgent.sources.Twitter.keywords ' equal to trump, primaries, immigration, jobs,
environment, Donald Trump.

Executing the Stream
We can start the process by browsing to our Flume home directory in the terminal and and start
fetching the data from twitter by executing the application as following
$ cd $FLUME_HOME
$ bin/flume-ng agent --conf ./conf/ -f conf/twitter.conf
Dflume.root.logger=DEBUG,console -n TwitterAgent
Once the process start running, we should start to see files showing up in our /user/flume/tweets
directory. As more events are processed, Flume writes to files in the appropriate directory. A temporary
file, suffixed with .tmp, is the file currently being written to. That .tmp suffix is removed when Flume
determines that the file contains enough events or enough time has passed to roll the file. Those
thresholds are determined in the configuration of the HDFS sink, as we saw above, by the rollCount
and rollInterval parameters, respectively.
If everything goes fine, the Flume middleware will start streaming of tweets into HDFS. We wait for
20-30 seconds and let flume stream the data on HDFS, after that we break the process by pressing ctrl
+ c and stop the streaming. Since we will be stopping the process, there will be few exceptions thrown
by the middleware. Given below in figure 4.3 is the snapshot of the command prompt window while
fetching tweets.
![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/12.PNG)

We can view our stored and localized tweets by accessing the Hadoop cluster administration web UI
using the URL given below.
http://localhost:50070/
Click on the dropdown named Utilities on the right-hand side of the page and click on Browse the File
System. You can see menu and options as shown in figure 4.4 given below.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/13.PNG)

After clicking on Browse file system we navigate to the path of the HDFS directory where we have
stored the tweets. In our example, the path will be /hadoop/elections/. Then, you can see the list of
twitter log files stored in HDFS as shown in figure 4.5 and 4.6 below.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/14.PNG)
![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/15.PNG)

By clicking on the log files, a new dialog box is created as shown in figure 4.7 below. We can
download the log file data and use it in our analytics.

![alt text](https://github.com/msaadsadiq/msaadsadiq.github.io/blob/master/_posts/16.PNG)

Displayed below is a sample snippet from the log file indicated a tweeted messag wrapped in several
time, location and geo-relational tags. We can see the original embedded tweet highlighted for more
clarity.

Sample Data from Twitter Feed
{"in_reply_to_status_id_str":null,"in_reply_to_status_id":null,"created_at":"Fri Apr 22 22:47:00 +0000
2016","in_reply_to_user_id_str":null,"source":"<a href=\"http://www.tweetmyjobs.com\"
rel=\"nofollow\">TweetMyJOBS<\/a>","retweet_count":0,"retweeted":false,"geo":{"coordinates":[38.4661804,-
121.8207636],"type":"Point"},"filter_level":"low","in_reply_to_screen_name":null,"is_quote_status":false,"id_str":"723644
298604900352","in_reply_to_user_id":null,"favorite_count":0,"id":723644298604900352,"text":"Want to work at Tractor
Supply Company? We're #hiring in #Dixon, CA! Click for details: https://t.co/ALnj3Hv0zT #Retail #Job #Jobs","place":

{"country_code":"US","country":"United States","full_name":"Dixon, CA","bounding_box":{"coordinates":[[[-
121.862342,38.421895],[-121.862342,38.473182],[-121.805679,38.473182],[-
121.805679,38.421895]]],"type":"Polygon"},"friends_count":311,"profile_image_url_https":"https://pbs.twimg.com/profile
_images/668263058087677952/mcFqziT9_normal.jpg","listed_count":56,"profile_background_image_url":"http://pbs.twim

g.com/profile_background_images/315482597/Twitter-BG_2_bg-
image.jpg","default_profile_image":false,"favourites_count":0,"description":"Follow this account for geo-targeted Retail

job tweets in Sacramento, CA. Need help? Tweet us at @CareerArc!","created_at":"Fri Apr 03 22:53:42 +0000
2009","is_translator":false,"profile_background_image_url_https":"https://pbs.twimg.com/profile_background_images/315

482597/Twitter-BG_2_bg-
image.jpg","protected":false,"screen_name":"tmj_sac_retail","id_str":"28677207","profile_link_color":"4A913C","id":2867

7207,"geo_enabled":true,"profile_background_color":"253956","lang":"en","profile_sidebar_border_color":"000000","prof
ile_text_color":"000000","America First","location":null,"profile_sidebar_fill_color":"DDEEF6","notifications":null}}
{"in_reply_to_status_id_str":null,"in_reply_to_status_id":null,"created_at":"Fri Apr 22 22:47:00 +0000
2016","in_reply_to_user_id_str":null,"source":"<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web
Client<\/a>","retweet_count":0,"retweeted":false,"geo":null,"filter_level":"low","in_reply_to_screen_name":null,"is_quote_
status":false,"id_str":"723644299846381568","in_reply_to_user_id":null,"favorite_count":0,"id":723644299846381568,"te
xt":"Let's review When everone knew Trump was a #14ktfake conservative https://t.co/37GQ66S4qn","place":
{"country_code":"US","country":"United States","full_name":"Houston, TX","bounding_box":{"coordinates":[[[-
95.823268,29.522325],[-95.823268,30.154665],[-95.069705,30.154665],[-
95.069705,29.522325]]],"type":"Polygon"},"place_type":"city","name":"Houston","attributes":
{},"id":"1c69a67ad480e1b1"

If you can see data files similar to those shown in figure 4.6, then the unstructured twitter data has been
tranlated and streamed from twitter on to HDFS successfully using the Flume middleware. Now we can
perform analytics on this twitter data using Hive or any other analytical programming tool.

VI. Conclusion
With the advancement of software development, advent of cloud computing and the maturity of
programming languages, the world of computing and interacting with other computers is becoming
complex and heterogeneous. To communicated and transfer data between these disparate and highly
sophisticated technologies, it is necessary to employ middleware programs that speak languages of both
parties and know how to translate among them. One such scenario is the communication between social
media and big data storage component HDFS. Both technologies are very recent and are at the bleeding
edge of technological development. It is noteworthy to mention that both technologies are also very
experimental and no mature software platforms is available to act as a middleware between these
applications.
In this project, we have explored an open source experimental project by the Apache family known as
Flume to act as a middleware between social media giant Twitter and distributed data storage
technology HDFS. To achieve this we have built a virtually-distributed Hadoop cluster as the final data
storage unit and configured a Twitter API that will stream live data to our middleware program. It was
observed that huge amounts of twitter data can be streamed into the hadoop cluster using the
middleware program. We selected several elections 2016 based keywords to narrow down our twitter
search. Stored results were displayed in section V to have successfully streamed twitter data into
HDFS. This data can further be utilized by analytical tools to observe sentiments, value, and statistics.
This area of research has only slightly been tapped and has great potential for further research and
development. An example of this can be understood from the fact that Flume is the only available
middleware program for distributed storage technologies. Our aim is to further explore this area in the
perspective of multimedia big data and aim to improve the search and retrieval capabilities of the
system.
